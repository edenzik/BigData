package hadoop01.utils;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.util.HashSet;

/**
 * Utility class for parsing input for better clustering
 * Normalizes tokens (without stemming) and removes high and low frequency words
 * Stop words are pre-computed
 * @author Michael Partridge
 *
 */
public class StopFilter {

	/**
	 * Utility to parse Amazon input
	 * Sets all words to lower case and removes punctuation (by space-substitution)
	 * Splits all words on space into tokens
	 * Filters tokens by supplied stop word list (generated by frequency)
	 * Writes tokens space-separated back to output in same order
	 * 
	 * @param args [0]: List of stop words, [1]: Input data to parse, [2]: Output path
	 */
	public static void main(String[] args) {

		HashSet<String> stopSet = new HashSet<String>(100000);

		try {
			BufferedReader stopReader = new BufferedReader(new FileReader(args[0]));


			while (stopReader.ready()) {

				String next = stopReader.readLine();
				stopSet.add(next);	

			}
			stopReader.close();

			BufferedReader inputReader = new BufferedReader(new FileReader(args[1]));
			FileWriter writer = new FileWriter(args[2]);

			while (inputReader.ready()) {

				String nextLine = inputReader.readLine().toLowerCase();
				nextLine = nextLine.replaceAll("[^a-z ]", " ");
				for (String word : nextLine.split(" ")) {
					if (!word.equals(" ") && !word.equals("")) {
						writer.write(word + " ");
					}

				}

				writer.write("\n");
			}
			
			writer.flush();
			writer.close();
			inputReader.close();

		} catch (IOException e) {
			e.printStackTrace();
		}


	}

}
